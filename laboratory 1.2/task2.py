# -*- coding: utf-8 -*-
"""task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEyirfnM5e4f4tx4GrrpH5k6hPf1aomU

# **Лабараторная работа 2**

**Суть** - векторизовать контент статьи и кластеризовать данные

*  *Этап 1 - нормализация*

С помощью библиотеки natasha или pymorphy 2 - преобразовать текст в нормальную форму, выбросить стоп-слова


*  *Этап 2 - веткоризация*
Используйте 1 из представленных способов

CountVectorizer (sklearn)
TFIDFVectrorizer (sklearn)
Word2Vec (gensim)


 * *Этап 3 - Сжатие векторов*

С помощью PCA - сжать векторы до 2-точек, для построения кластеров


 * *Этап 4 - клатеризация*

использовать KMeans
Использовать DBSSCAN


 * *Этап 5 - результаты анализа*

Выделить темы, по которым были построены кластеры

# **Этап №1: Нормализация**
"""

!pip install natasha

import json
from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, Doc
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Загружаем JSON файл
with open('articles.json', 'r', encoding='utf-8') as f:
    articles = json.load(f)

# Инициализация Natasha компонентов
segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
syntax_parser = NewsSyntaxParser(emb)
ner_tagger = NewsNERTagger(emb)

def normalize_text(text):
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    doc.parse_syntax(syntax_parser)
    doc.tag_ner(ner_tagger)

    tokens = []
    for token in doc.tokens:
        token.lemmatize(morph_vocab)
        if token.lemma not in ENGLISH_STOP_WORDS and token.lemma.isalpha():
            tokens.append(token.lemma)
    return ' '.join(tokens)

# Нормализация всех статей
normalized_articles = [normalize_text(article['content']) for article in articles]
# Выводим несколько примеров нормализованных текстов
print("Примеры нормализованных текстов:")
for i, norm_text in enumerate(normalized_articles[:3]):
    print(f"Статья {i+1}: {norm_text}\n")

"""# **Этап №2: Векторизация**

Мы будем использовать TFIDFVectorizer из библиотеки sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(normalized_articles)

# Выводим примеры векторов
print("Примеры векторов TF-IDF (первые 3 статьи):")
print(X[:3].toarray())

# Выводим статистику по векторам
mean_vector = np.mean(X.toarray(), axis=0)
std_vector = np.std(X.toarray(), axis=0)
print("Среднее значение по векторам TF-IDF:")
print(mean_vector)
print("Стандартное отклонение по векторам TF-IDF:")
print(std_vector)

"""# **Этап №3: Сжатие векторов**

Используем PCA для уменьшения размерности векторов до 2.
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X.toarray())

# Выводим сжатые векторы
print("Примеры сжатых векторов после PCA (первые 3 статьи):")
print(X_reduced[:3])

# Выводим объясненную дисперсию для компонентов PCA
explained_variance = pca.explained_variance_ratio_
print("Объясненная дисперсия для компонентов PCA:")
print(explained_variance)
print(f"Суммарная объясненная дисперсия: {np.sum(explained_variance)}")

"""# **Этап №4: Кластеризация**

Мы будем использовать KMeans и DBSCAN из библиотеки sklearn для кластеризации данных.
"""

from sklearn.cluster import KMeans, DBSCAN
import matplotlib.pyplot as plt

# KMeans кластеризация
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans_labels = kmeans.fit_predict(X_reduced)

# DBSCAN кластеризация
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_reduced)

# Визуализация кластеров
def plot_clusters(X, labels, title):
    plt.figure(figsize=(10, 7))
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
    plt.title(title)
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.colorbar()
    plt.show()

print("KMeans Кластеры:")
plot_clusters(X_reduced, kmeans_labels, 'KMeans Clusters')

print("DBSCAN Кластеры:")
plot_clusters(X_reduced, dbscan_labels, 'DBSCAN Clusters')

"""# **Этап №5: Результаты анализа**"""

import numpy as np
import pandas as pd

def get_top_terms_per_cluster(vectorizer, X, labels, n_terms=10):
    df = pd.DataFrame(X.todense()).groupby(labels).mean()
    terms = vectorizer.get_feature_names_out()
    for i, row in df.iterrows():
        print(f"\nКластер {i}")
        print(', '.join([terms[t] for t in np.argsort(row)[-n_terms:]]))

print("Ключевые слова для каждого кластера (KMeans):")
get_top_terms_per_cluster(vectorizer, X, kmeans_labels)